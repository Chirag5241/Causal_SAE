{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire, os, sys, random\n",
    "import torch, json\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM\n",
    "# from src.token_util import tokenizeData\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Note: causal1 and causal2 are not different causal variables, rather different values of the same causal varible.\n",
    "#   Bear this in mind when doing tasks with more causal variables.........\n",
    "\n",
    "# If variables aren't split up by outcomes of causal variable, then it only has 'causal'.\n",
    "\n",
    "def clean_data(data, data_name):\n",
    "    '''\n",
    "    For Linzen marvin_linzen and BERT tokenizer.\n",
    "    Remove coordinating examples and verbs where both forms not in vocabulary.\n",
    "    '''\n",
    "    for k in data[0].keys(): assert k in {'other', 'causal', 'causal1', 'causal2', 'envir1'}\n",
    "    if 'causal' in data[0].keys(): assert 'causal1' not in data[0].keys() and 'causal2' not in data[0].keys()\n",
    "    if 'causal1' in data[0].keys() or 'causal2' in data[0].keys(): assert 'causal' not in data[0].keys() and 'causal1' in data[0].keys() and 'causal2' in data[0].keys()\n",
    "    print(\"Length of data:\", len(data))\n",
    "\n",
    "    if data_name == 'marvin_linzen':\n",
    "        # No coordination\n",
    "        data = [x for x in data if 'coord' not in x['other']['sent_type']]\n",
    "        print(\"Length of data with no coordination:\", len(data))\n",
    "\n",
    "    if data_name == 'marvin_linzen': pair_extractor = lambda ex: (ex['causal1']['trg_wd'], ex['causal1']['trg_wd_flip'])\n",
    "    elif data_name in {'lgd', 'lgd_orig'}: pair_extractor = lambda ex: (ex['causal']['trg_wd'], ex['causal']['trg_wd_flip']) if ex['causal']['label'] else (ex['causal']['trg_wd_flip'], ex['causal']['trg_wd'])\n",
    "    else: assert False\n",
    "\n",
    "    # Count verb pairs\n",
    "    verb_pairs = {}\n",
    "    for x in data:\n",
    "        pair = pair_extractor(x)\n",
    "        verb_pairs[pair] = verb_pairs.get(pair, 0) + 1\n",
    "\n",
    "    # Remove invalid verbs\n",
    "    verb_pairs_dct = {}\n",
    "    print('Verb pairs:')\n",
    "\n",
    "    # temp_vocab_list = [\"is\", \"are\", \"has\", \"have\"]\n",
    "    temp_vocab_list = [\"swim\", \"swims\", \"smile\", \"smiles\", \"brings\", \"bring\", \"interests\", \"interest\"]\n",
    "\n",
    "    for x in verb_pairs:\n",
    "        boole = x[0] not in temp_vocab_list and x[1] not in temp_vocab_list\n",
    "        print('\\t', x, boole, '- will remove sentences' if not boole else '', '-', verb_pairs[x])\n",
    "        verb_pairs_dct[x] = boole\n",
    "\n",
    "    data = [x for x in data if verb_pairs_dct[pair_extractor(x)]]\n",
    "    print('Length of data after removing invalid verbs:', len(data))\n",
    "\n",
    "    # Confirm there's no duplicate data w.r.t. the input sentence-output pair.\n",
    "    has_causal_by_value = 'causal1' in data[0]\n",
    "    sentwd_extractor = lambda ex, var: (ex[var]['mask'], ex[var]['trg_wd'])\n",
    "    sents_causal1, sents_causal2 = {}, {}\n",
    "    sents_causal1_without_vb, sents_causal2_without_vb = {}, {}\n",
    "    for x in data:\n",
    "        causal1_key = sentwd_extractor(x, 'causal1' if has_causal_by_value else 'causal')\n",
    "        causal2_key = sentwd_extractor(x, 'causal2') if has_causal_by_value else None\n",
    "        mask_causal1, mask_causal2 = causal1_key[0], causal2_key[0] if has_causal_by_value else None\n",
    "\n",
    "        sents_causal1[causal1_key] = sents_causal1.get(causal1_key, 0) + 1\n",
    "        sents_causal2[causal2_key] = sents_causal2.get(causal2_key, 0) + 1\n",
    "        sents_causal1_without_vb[mask_causal1] = sents_causal1_without_vb.get(mask_causal1, 0) + 1\n",
    "        sents_causal2_without_vb[mask_causal2] = sents_causal2_without_vb.get(mask_causal2, 0) + 1\n",
    "    if has_causal_by_value: assert len(sents_causal1) == len(sents_causal2) and len(sents_causal1_without_vb) == len(sents_causal2_without_vb)\n",
    "    if has_causal_by_value: assert len(sents_causal1_without_vb) == len(sents_causal2_without_vb)\n",
    "    assert len(sents_causal1) == len(data), 'There is duplicate data!!!'\n",
    "\n",
    "    if len(sents_causal1_without_vb) != len(data): print(\"WARNING: There are sentences that are the same input with [MASK],\"\n",
    "                                                    \" but the overall target verb (i.e. prediction) is different. Number of unique sentences\"\n",
    "                                                    \" up to verb (i.e. unique inputs) is\", len(sents_causal1_without_vb))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def getMaskVectorsAtLayerForOneBatch(batch_tokens, batch_mask_idxes, model, layer):\n",
    "    '''\n",
    "    Run examples through LLM for one batch.\n",
    "    '''\n",
    "    assert layer == -1, 'only last layer for now'\n",
    "\n",
    "    # Get embeddings by running through LLM\n",
    "    attention_mask = (batch_tokens != 0).float().to(batch_tokens.device)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_tokens, attention_mask=attention_mask) # (batch_size, max_input_len, embed_dim)\n",
    "    embeds = output['hidden_states'][layer]\n",
    "\n",
    "    # Now select the vector corresponding to the mask token\n",
    "    mask_embeds = torch.gather(embeds, 1, batch_mask_idxes.view(-1, 1, 1).expand(-1, 1, embeds.shape[\n",
    "        2]))  # (batch_size, 1, embed_dim)\n",
    "    mask_embeds = mask_embeds.squeeze(1)  # (batch_size, embed_dim)\n",
    "\n",
    "    return mask_embeds\n",
    "\n",
    "\n",
    "def getMaskVectorsAtLayer(tokens, mask_idxes, batch_size, model, layer, do_tqdm):\n",
    "    # assert tokens.shape[0] % batch_size == 0 # For now, just use even data\n",
    "    embeddings = torch.zeros(tokens.shape[0], 768, device=tokens.device)\n",
    "    iterator = range(0, tokens.shape[0], batch_size)\n",
    "    iterator = tqdm(iterator) if do_tqdm else iterator\n",
    "    for start_idx in iterator:\n",
    "        if not do_tqdm and random.random() < 0.25: print (start_idx, '/', tokens.shape[0]) # random to reduce pritning a bit\n",
    "        embeds = getMaskVectorsAtLayerForOneBatch(tokens[start_idx:(\n",
    "            start_idx+batch_size),], mask_idxes[start_idx:(start_idx+batch_size)], model, layer)\n",
    "        embeddings[start_idx:(start_idx+batch_size),] = embeds\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run(data_file,\n",
    "        num_examples=None,\n",
    "        batch_size=200,\n",
    "        do_tqdm=True):\n",
    "    UNCLEAN_DATA = ('' if 'data' in os.listdir() else '../') + 'data/'+data_file+'.json'\n",
    "    ARTEFACT_PATH = ('' if 'pipeline_artefacts' in os.listdir() else '../') + 'pipeline_artefacts/' + data_file + '/'\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('DEVICE:', DEVICE)\n",
    "    \n",
    "    if not os.path.exists(ARTEFACT_PATH):\n",
    "        os.makedirs(ARTEFACT_PATH)\n",
    "\n",
    "    # Read and clean the data\n",
    "    with open(UNCLEAN_DATA, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = clean_data(data, data_file)[:num_examples] # See function for what it does\n",
    "    \n",
    "    json_object = json.dumps(data)\n",
    "    with open(ARTEFACT_PATH+'s1_specific_data_clean_ma_lin.json', 'w', newline='') as f:\n",
    "        f.write(json_object)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Length of data: 63130\n",
      "Length of data with no coordination: 62510\n",
      "Verb pairs:\n",
      "\t ('laughs', 'laugh') True  - 3730\n",
      "\t ('swims', 'swim') False - will remove sentences - 3730\n",
      "\t ('smiles', 'smile') False - will remove sentences - 3730\n",
      "\t ('is', 'are') True  - 24920\n",
      "\t ('brings', 'bring') False - will remove sentences - 2000\n",
      "\t ('interests', 'interest') False - will remove sentences - 2000\n",
      "\t ('likes', 'like') True  - 5600\n",
      "\t ('admires', 'admire') True  - 5600\n",
      "\t ('hates', 'hate') True  - 5600\n",
      "\t ('loves', 'love') True  - 5600\n",
      "Length of data after removing invalid verbs: 51050\n",
      "WARNING: There are sentences that are the same input with [MASK], but the overall target verb (i.e. prediction) is different. Number of unique sentences up to verb (i.e. unique inputs) is 34250\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "run(\"marvin_linzen\",num_examples=None,batch_size=200,do_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
